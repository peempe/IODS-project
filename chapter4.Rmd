# Clustering and classification

### Overview of the data

*The dataset is an example dataset in package MASS. It includes information of housing values in suburbs of Boston*

*More information about the dataset can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).*

```{r}
library(magrittr)
library(MASS)

# Load the data
data("Boston")

# Explore the dataset
str(Boston) # 506 rows/obervations, 14 columns/variables
summary(Boston)

# Calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# Print the correlation matrix
cor_matrix

# Visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

The correlation matrix gives us a nice overview of the relationships between the variables. By using colors to indicate the direction and strength of the correlation, we can see the relevant information with quick look. The matrix shows that the strongest negative correlation is between variables "rad" and "tax". This indicates that higher index of accessibility to radial highways is associated with lower full-value property-tax rate per \$10,000. The strong positive correlation can be found e.g. between variables "age" and "dis" addressing that higher proportion of owner-occupied units built prior to 1940 is associated with higher weighted mean of distances to five Boston employment centres. 


### Scaling the dataset

```{r}
# Center and standardize variables
boston_scaled <- scale(Boston)

# Summaries of the scaled variables
summary(boston_scaled)

# Class of the boston_scaled object
class(boston_scaled)

# Change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Scaling of the dataset standardize the variables so that the mean is 0 and standard deviation is 1. 


### Creating a categorical variable of the crime rate

```{r}
# Summary of the scaled crime rate
summary(boston_scaled$crim)

# Create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# Create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# Look at the table of the new factor crime
table(crime)

# Remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# Add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```


### Dividing the datset to a train and test sets

```{r}
# Number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# Choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Create train set
train <- boston_scaled[ind,]

# Create test set 
test <- boston_scaled[-ind,]
```


### Linear discriminant analysis (LDA)

```{r}
# Linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# Print the lda.fit object
lda.fit

# The function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# Target classes as numeric
classes <- as.numeric(train$crime)

# Plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)

# Save the correct classes from test data
correct_classes <- test$crime

# Remove the crime variable from test data
test <- dplyr::select(test, -crime)

# Predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

LDA model works quite well. As assumed, the contiguous categories are harder to discriminate. However, the model does not mix extremity categories themselves at all.


### Distances and K-means

```{r}
# Reload the data and standardize it
data("Boston")
boston_scaled <- scale(Boston)

# Euclidean distance matrix
dist_eu <- dist(Boston)

# Look at the summary of the distances
summary(dist_eu)

# Manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# Look at the summary of the distances
summary(dist_man)

# K-means clustering
km <-kmeans(Boston, centers = 3)

# Plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)

set.seed(123)

# Determine the number of clusters
k_max <- 10

# Calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# Visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

As can be seen from the total WCSS picture, the value of total WCSS radically changes between 1 and 2,5. It indicates that the optimal number of clusters could be 2. Let's run the algorithm again.

```{r}
# K-means clustering
km <-kmeans(Boston, centers = 2)

# Plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

Now the colors red and black visualize the two clusters.